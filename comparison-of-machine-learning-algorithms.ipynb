{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2603715,"sourceType":"datasetVersion","datasetId":1582403}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-08T13:55:08.843080Z","iopub.execute_input":"2025-09-08T13:55:08.843403Z","iopub.status.idle":"2025-09-08T13:55:08.861560Z","shell.execute_reply.started":"2025-09-08T13:55:08.843380Z","shell.execute_reply":"2025-09-08T13:55:08.860617Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load the dataset","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/heart-failure-prediction/heart.csv\")\ndata.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T13:55:08.874484Z","iopub.execute_input":"2025-09-08T13:55:08.874832Z","iopub.status.idle":"2025-09-08T13:55:08.892548Z","shell.execute_reply.started":"2025-09-08T13:55:08.874805Z","shell.execute_reply":"2025-09-08T13:55:08.891430Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.tail()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T13:55:08.946885Z","iopub.execute_input":"2025-09-08T13:55:08.947950Z","iopub.status.idle":"2025-09-08T13:55:08.960264Z","shell.execute_reply.started":"2025-09-08T13:55:08.947912Z","shell.execute_reply":"2025-09-08T13:55:08.959345Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T13:55:08.961792Z","iopub.execute_input":"2025-09-08T13:55:08.962174Z","iopub.status.idle":"2025-09-08T13:55:08.982719Z","shell.execute_reply.started":"2025-09-08T13:55:08.962148Z","shell.execute_reply":"2025-09-08T13:55:08.981739Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T13:55:08.983518Z","iopub.execute_input":"2025-09-08T13:55:08.983766Z","iopub.status.idle":"2025-09-08T13:55:08.999970Z","shell.execute_reply.started":"2025-09-08T13:55:08.983741Z","shell.execute_reply":"2025-09-08T13:55:08.999025Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"missing_values = data.isnull().sum()\nprint(missing_values)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T13:55:09.002048Z","iopub.execute_input":"2025-09-08T13:55:09.002355Z","iopub.status.idle":"2025-09-08T13:55:09.020820Z","shell.execute_reply.started":"2025-09-08T13:55:09.002334Z","shell.execute_reply":"2025-09-08T13:55:09.019963Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Pre-processing","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX = data.drop('HeartDisease', axis=1)\ny = data['HeartDisease']\n\nX = pd.get_dummies(X, drop_first=True)\n\nprint(\"Data types BEFORE conversion:\")\nprint(X.dtypes)\n\n# Identify non-numeric columns (should be none after get_dummies, but let's check)\nnon_numeric_cols = X.select_dtypes(include=['object', 'category']).columns\nprint(f\"\\nNon-numeric columns found: {list(non_numeric_cols)}\")\n\n# If there are any non-numeric columns, we need to convert them.\n# This often happens if get_dummies doesn't work as expected or a column has mixed types.\n# The safest way is to force conversion to numeric, coering errors to NaN.\nfor col in X.columns:\n    # Convert each column to numeric, forcing any unconvertible values to NaN\n    X[col] = pd.to_numeric(X[col], errors='coerce')\n\n# Now, check if the conversion created any NaN values and handle them\nmissing_after_conversion = X.isnull().sum().sum()\nif missing_after_conversion > 0:\n    print(f\"\\nWARNING: Conversion created {missing_after_conversion} NaN values.\")\n    # Print which columns have NaNs\n    print(\"NaN values per column:\")\n    print(X.isnull().sum())\n    # Simple strategy: fill NaNs with the column mean (or median, or mode for categorical)\n    # Since this is likely an error, we use mean for simplicity.\n    X = X.fillna(X.mean())\n    print(\"\\nNaN values have been filled with column means.\")\n\nprint(\"\\nData types AFTER conversion:\")\nprint(X.dtypes)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nprint(\"X_train shape:\", X_train.shape)\nprint(\"X_test shape:\", X_test.shape)\nprint(\"y_train shape:\", y_train.shape)\nprint(\"y_test shape:\", y_test.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T13:55:09.021692Z","iopub.execute_input":"2025-09-08T13:55:09.022024Z","iopub.status.idle":"2025-09-08T13:55:09.056760Z","shell.execute_reply.started":"2025-09-08T13:55:09.022003Z","shell.execute_reply":"2025-09-08T13:55:09.055819Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training and Testing","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\n\n# Initialize the scaler\nscaler = StandardScaler()\n\n# Fit on training data, transform both training and testing data\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test) # Note: Only transform the test data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T13:55:09.057714Z","iopub.execute_input":"2025-09-08T13:55:09.058463Z","iopub.status.idle":"2025-09-08T13:55:09.072053Z","shell.execute_reply.started":"2025-09-08T13:55:09.058442Z","shell.execute_reply":"2025-09-08T13:55:09.071017Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"After preparing the data, we will now apply different Machine learning algorithms to test the accuracy and make comparisons between them using the same dataset.","metadata":{}},{"cell_type":"markdown","source":"# **1. Logisitic Regression**","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlog_reg = LogisticRegression(random_state=42, max_iter=1000)\nlog_reg.fit(X_train, y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T13:55:09.073227Z","iopub.execute_input":"2025-09-08T13:55:09.073521Z","iopub.status.idle":"2025-09-08T13:55:10.805449Z","shell.execute_reply.started":"2025-09-08T13:55:09.073500Z","shell.execute_reply":"2025-09-08T13:55:10.804648Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, roc_curve, auc, confusion_matrix, RocCurveDisplay\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Make predictions on the test set\ny_pred = log_reg.predict(X_test)\ny_prob = log_reg.predict_proba(X_test)[:, 1]\n\n# Calculate and print accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.2f}\")\n\n# Generate and display the confusion matrix\nconf_matrix = confusion_matrix(y_test, y_pred)\nplt.figure(figsize=(6, 4))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.show()\n\n# Compute ROC curve and AUC\nfpr, tpr, thresholds = roc_curve(y_test, y_prob)\nroc_auc = auc(fpr, tpr)\n\n# Display ROC curve\nplt.figure(figsize=(8, 6))\nroc_display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='Logistic Regression')\nroc_display.plot()\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T13:55:10.807634Z","iopub.execute_input":"2025-09-08T13:55:10.807970Z","iopub.status.idle":"2025-09-08T13:55:11.208994Z","shell.execute_reply.started":"2025-09-08T13:55:10.807937Z","shell.execute_reply":"2025-09-08T13:55:11.208094Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Key Findings from Data Analysis **\n\n1. The logistic regression model demonstrated an accuracy rate of 85% on the validation set.  \n2. The model's Area Under the ROC Curve (AUC) was quite high, reflecting exceptional ability to differentiate between patients who do and do not have heart disease.  \n3. The most significant factors influencing the prediction of heart disease, as indicated by the model's coefficients, include ChestPainType_NAP, ChestPainType_ATA, ST_Slope_Flat, Sex_M, and ExerciseAngina_Y.","metadata":{}},{"cell_type":"markdown","source":"**ACCURACY - 85%**","metadata":{}},{"cell_type":"markdown","source":"# **2. Decision Tree**","metadata":{}},{"cell_type":"code","source":"# Pipeline\nfrom sklearn.pipeline import Pipeline \nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import cross_val_score\n\n# Model\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n\nimport warnings \nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T13:55:11.209869Z","iopub.execute_input":"2025-09-08T13:55:11.210265Z","iopub.status.idle":"2025-09-08T13:55:11.215622Z","shell.execute_reply.started":"2025-09-08T13:55:11.210241Z","shell.execute_reply":"2025-09-08T13:55:11.214891Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x_values = data.select_dtypes(include=['number'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T13:55:11.216776Z","iopub.execute_input":"2025-09-08T13:55:11.217253Z","iopub.status.idle":"2025-09-08T13:55:11.294486Z","shell.execute_reply.started":"2025-09-08T13:55:11.217228Z","shell.execute_reply":"2025-09-08T13:55:11.293469Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(18, 8))\nsns.countplot(data=data, x='HeartDisease', palette='YlOrRd')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T13:55:11.295488Z","iopub.execute_input":"2025-09-08T13:55:11.295810Z","iopub.status.idle":"2025-09-08T13:55:11.450131Z","shell.execute_reply.started":"2025-09-08T13:55:11.295783Z","shell.execute_reply":"2025-09-08T13:55:11.449089Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Separation of features and target\n\nX = data.drop('HeartDisease',axis=1)\ny = data['HeartDisease']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,stratify=y)\n\nnumerical_columns = list(X_train.select_dtypes(include=['float64', 'int64']).columns)\ncategorical_columns = list(X_train.select_dtypes(include=['object', 'category']).columns)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T13:55:11.451112Z","iopub.execute_input":"2025-09-08T13:55:11.451360Z","iopub.status.idle":"2025-09-08T13:55:11.462135Z","shell.execute_reply.started":"2025-09-08T13:55:11.451339Z","shell.execute_reply":"2025-09-08T13:55:11.461196Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Pipeline\nnum_pipeline = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', StandardScaler())\n])\n\ncat_pipeline = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n])\n\npreprocessor = ColumnTransformer(transformers=[\n    ('num', num_pipeline, numerical_columns),\n    ('cat', cat_pipeline, categorical_columns)\n])\n\npipeline = Pipeline(steps=[\n    ('preprocessing', preprocessor),\n    ('DecisionTree', DecisionTreeClassifier())\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T13:55:11.463104Z","iopub.execute_input":"2025-09-08T13:55:11.463418Z","iopub.status.idle":"2025-09-08T13:55:11.475179Z","shell.execute_reply.started":"2025-09-08T13:55:11.463377Z","shell.execute_reply":"2025-09-08T13:55:11.474118Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training pipeline\npipeline.fit(X_train, y_train)\ny_pred = pipeline.predict(X_test)\nscores = cross_val_score(pipeline, X, y, cv=5, scoring='accuracy')\n\n\n\nprint(classification_report(y_test, y_pred))\nprint(f\"Cross Validation: {scores.mean():.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T13:55:11.476244Z","iopub.execute_input":"2025-09-08T13:55:11.476488Z","iopub.status.idle":"2025-09-08T13:55:11.621050Z","shell.execute_reply.started":"2025-09-08T13:55:11.476468Z","shell.execute_reply":"2025-09-08T13:55:11.620179Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.tree import plot_tree\n\nmodelo = pipeline.named_steps['DecisionTree']\nfeature_names_num = numerical_columns\ncat_encoder = pipeline.named_steps['preprocessing'].named_transformers_['cat'].named_steps['encoder']\ncat_features = cat_encoder.get_feature_names_out(categorical_columns)\nfeature_names = list(feature_names_num) + list(cat_features)\n\n# 3. Plotar a Ã¡rvore\nplt.figure(figsize=(20, 10))\nplot_tree(modelo, \n          filled=True, \n          feature_names=feature_names, \n          class_names=[str(cls) for cls in modelo.classes_])\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T13:55:11.622031Z","iopub.execute_input":"2025-09-08T13:55:11.622350Z","iopub.status.idle":"2025-09-08T13:55:17.146297Z","shell.execute_reply.started":"2025-09-08T13:55:11.622321Z","shell.execute_reply":"2025-09-08T13:55:17.145318Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Key Findings from Data Analysis**\n\n1. The Decision Tree model demonstrated an accuracy rate of 74% on the validation set.\n2. One hot encoder is used to balance the string values.\n","metadata":{}},{"cell_type":"markdown","source":"**ACCURACY = 74%**","metadata":{}},{"cell_type":"markdown","source":"# **3. Support Vector Machine**","metadata":{}},{"cell_type":"code","source":"# Data Handling\nimport pandas as pd\nimport numpy as np\n\n# Train-Test Split & Preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\n\n# Models\nfrom sklearn.svm import SVC\n\n# Evaluation Metrics\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.metrics import silhouette_score  # For evaluating clusters\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T13:55:17.147371Z","iopub.execute_input":"2025-09-08T13:55:17.147673Z","iopub.status.idle":"2025-09-08T13:55:17.153768Z","shell.execute_reply.started":"2025-09-08T13:55:17.147650Z","shell.execute_reply":"2025-09-08T13:55:17.152968Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\n\" + \"=\"*50)\nprint(\"SUPPORT VECTOR MACHINE (SVM)\")\nprint(\"=\"*50)\n\n# Create and train the model on SCALED data\nsvm_model = SVC(kernel='rbf', random_state=42) # RBF kernel is a good default\nsvm_model.fit(X_train_scaled, y_train)\n\n# Make predictions\ny_pred_svm = svm_model.predict(X_test_scaled)\n\n# Evaluate\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred_svm).round(4))\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred_svm))\nprint(\"\\nConfusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred_svm))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T13:55:17.154954Z","iopub.execute_input":"2025-09-08T13:55:17.155272Z","iopub.status.idle":"2025-09-08T13:55:17.216770Z","shell.execute_reply.started":"2025-09-08T13:55:17.155243Z","shell.execute_reply":"2025-09-08T13:55:17.215879Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**ACCURACY - 54.35%**","metadata":{}},{"cell_type":"markdown","source":"# **4. KNN**","metadata":{}},{"cell_type":"code","source":"# Data Handling\nimport pandas as pd\nimport numpy as np\n\n# Train-Test Split & Preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\n\n# Models\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Evaluation Metrics\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.metrics import silhouette_score  # For evaluating clusters\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T13:55:17.219864Z","iopub.execute_input":"2025-09-08T13:55:17.220345Z","iopub.status.idle":"2025-09-08T13:55:17.225022Z","shell.execute_reply.started":"2025-09-08T13:55:17.220321Z","shell.execute_reply":"2025-09-08T13:55:17.224175Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\n\" + \"=\"*50)\nprint(\"K-NEAREST NEIGHBORS (KNN)\")\nprint(\"=\"*50)\n\n# Create and train the model on SCALED data\nknn_model = KNeighborsClassifier(n_neighbors=5) # You can tune this parameter\nknn_model.fit(X_train_scaled, y_train)\n\n# Make predictions\ny_pred_knn = knn_model.predict(X_test_scaled)\n\n# Evaluate\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred_knn).round(4))\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred_knn))\nprint(\"\\nConfusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred_knn))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T13:55:17.225870Z","iopub.execute_input":"2025-09-08T13:55:17.226123Z","iopub.status.idle":"2025-09-08T13:55:17.278488Z","shell.execute_reply.started":"2025-09-08T13:55:17.226096Z","shell.execute_reply":"2025-09-08T13:55:17.277662Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**ACCURACY - 50%**","metadata":{}},{"cell_type":"markdown","source":"# **5. PCA (Dimension Reduction)**","metadata":{}},{"cell_type":"markdown","source":"Principal Component Analysis","metadata":{}},{"cell_type":"code","source":"# Data Handling\nimport pandas as pd\nimport numpy as np\n\n# Train-Test Split & Preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\n\n# Models\nfrom sklearn.decomposition import PCA\n\n# Evaluation Metrics\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.metrics import silhouette_score  # For evaluating clusters\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T14:04:10.177969Z","iopub.execute_input":"2025-09-08T14:04:10.178343Z","iopub.status.idle":"2025-09-08T14:04:10.184024Z","shell.execute_reply.started":"2025-09-08T14:04:10.178319Z","shell.execute_reply":"2025-09-08T14:04:10.183035Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\n\" + \"=\"*50)\nprint(\"PRINCIPAL COMPONENT ANALYSIS (PCA)\")\nprint(50)\n\n# Apply PCA to the scaled data - use the same data that was used for scaling\npca = PCA(n_components=0.95) # Keep 95% of the variance\n\n# Fit on the TRAINING scaled data and transform both training and testing\nX_train_pca = pca.fit_transform(X_train_scaled)\nX_test_pca = pca.transform(X_test_scaled)\n\nprint(f\"Original number of features: {X_train_scaled.shape[1]}\")\nprint(f\"Reduced number of components: {X_train_pca.shape[1]}\")\n\n# Plot the explained variance ratio\nplt.figure(figsize=(10, 6))\nplt.bar(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_)\nplt.xlabel('Principal Component')\nplt.ylabel('Explained Variance Ratio')\nplt.title('PCA: Explained Variance by Component')\nplt.grid(True)\nplt.show()\n\n# Optional: Train a model on PCA-reduced data to see the impact\nsvm_pca_model = SVC(random_state=42)\nsvm_pca_model.fit(X_train_pca, y_train)\ny_pred_svm_pca = svm_pca_model.predict(X_test_pca)\n\nprint(\"\\nSVM Performance on PCA-Reduced Data:\")\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred_svm_pca).round(4))\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred_svm_pca))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T14:08:42.111342Z","iopub.execute_input":"2025-09-08T14:08:42.111682Z","iopub.status.idle":"2025-09-08T14:08:42.436337Z","shell.execute_reply.started":"2025-09-08T14:08:42.111656Z","shell.execute_reply":"2025-09-08T14:08:42.435428Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**ACCURACY - 53.8%**","metadata":{}},{"cell_type":"markdown","source":"# **Comparison**","metadata":{}},{"cell_type":"code","source":"# Import the plotting library\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# 1. Define the data\nmodels = ['Logistic Regression', 'Decision Tree', 'SVM', 'KNN', 'SVM (PCA)']\naccuracy = [85.0, 74.0, 54.35, 50.0, 53.8]  # Your accuracy values\ncolors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D', '#6B8F71']  # A professional color palette\n\n# 2. Create the figure and axis\nplt.figure(figsize=(10, 6))\nbars = plt.bar(models, accuracy, color=colors, edgecolor='black', linewidth=0.8, alpha=0.85)\n\n# 3. Customize the chart for clarity and professionalism\nplt.title('Comparison of Model Accuracies for Heart Disease Prediction', fontsize=14, fontweight='bold', pad=20)\nplt.ylabel('Accuracy (%)', fontsize=12, fontweight='bold')\nplt.xlabel('Machine Learning Models', fontsize=12, fontweight='bold')\nplt.ylim(0, 100)  # Set y-axis from 0 to 100% for accuracy\n\n# 4. Add the accuracy value on top of each bar\nfor bar, acc in zip(bars, accuracy):\n    height = bar.get_height()\n    plt.text(bar.get_x() + bar.get_width()/2., height + 1,\n             f'{acc}%', ha='center', va='bottom', fontweight='bold')\n\n# 5. Add a horizontal line at 50% (random guess baseline for binary classification)\nplt.axhline(y=50, color='r', linestyle='--', linewidth=1.2, alpha=0.7, label='Random Guess (50%)')\nplt.legend()\n\n# 6. Improve layout and display\nplt.grid(axis='y', alpha=0.3, linestyle=':')\nplt.xticks(rotation=45, ha='right') # Rotate model names if they are long\nplt.tight_layout() # Ensures everything fits in the figure\n\n# 7. Show the plot\nplt.show()\n\n# Optional: Save the figure as a high-resolution image for your article\n# plt.savefig('model_accuracy_comparison.png', dpi=300, bbox_inches='tight')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T14:36:47.531658Z","iopub.execute_input":"2025-09-08T14:36:47.533520Z","iopub.status.idle":"2025-09-08T14:36:47.816349Z","shell.execute_reply.started":"2025-09-08T14:36:47.533482Z","shell.execute_reply":"2025-09-08T14:36:47.815280Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Conclusion**","metadata":{}},{"cell_type":"markdown","source":"The above analysis demonstrates the accuracy of the different machine learning algorithms implementing on the same dataset.","metadata":{}}]}